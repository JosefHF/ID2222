{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "\n",
    "#Configure pyspark\n",
    "conf = SparkConf().setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['25',\n",
       "  '52',\n",
       "  '164',\n",
       "  '240',\n",
       "  '274',\n",
       "  '328',\n",
       "  '368',\n",
       "  '448',\n",
       "  '538',\n",
       "  '561',\n",
       "  '630',\n",
       "  '687',\n",
       "  '730',\n",
       "  '775',\n",
       "  '825',\n",
       "  '834'],\n",
       " ['39', '120', '124', '205', '401', '581', '704', '814', '825', '834']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the text file as an RDD\n",
    "rdd = sc.textFile('T10I4D100K.dat')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = rdd.count()\n",
    "\n",
    "#Flatten the rows into one list with every item\n",
    "#Filter out the empty strings\n",
    "flattened_rdd = rdd.flatMap(lambda line: line.split(\" \")).filter(lambda x: x not in '')\n",
    "\n",
    "#Reduce by key (item), count the occurences\n",
    "reduced_rdd = flattened_rdd.map(lambda x: (int(x),1)).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "#Filter out the items that appear in < 1% of the baskets\n",
    "filtered_rdd = reduced_rdd.filter(lambda x: x[1] > count*0.01)\n",
    "filtered_rdd.sortByKey(True).take(5)\n",
    "\n",
    "#Save frequent items L1\n",
    "frequent_items = np.zeros(count + 1)\n",
    "for elem in filtered_rdd.collect():\n",
    "    frequent_items[elem[0]] = 1\n",
    "    \n",
    "def extractDoubletons(doc):\n",
    "    pairs = []\n",
    "    for i in range(0,len(doc)-1):\n",
    "        for k in range(i+1,len(doc)-1):\n",
    "            if frequent_items[int(doc[i])] == 1 and frequent_items[int(doc[i+1])] == 1:\n",
    "                pairs.append(doc[i] + \" \" + doc[k])\n",
    "    return pairs\n",
    "test = rdd.map(lambda line: line.split(\" \")[0:len(line.split(\" \")) - 1])\n",
    "test2 = test.map(extractDoubletons)\n",
    "test3 = test2.flatMap(lambda x: x).map(lambda x: (x,1))\n",
    "L2 = test3.reduceByKey(lambda a,b: a+b).cache()\n",
    "L2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('217 346', 1188)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
